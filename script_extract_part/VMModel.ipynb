{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67369e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, heapq\n",
    "from operator import itemgetter\n",
    "\n",
    "class VSModel: \n",
    "    def __init__(self, movie_scripts, preprocessor):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.doc_len = len(movie_scripts)\n",
    "        self.index = self.get_inverted_index(movie_scripts)\n",
    "        print(len(self.index))\n",
    "    \n",
    "    def script_to_key_list(self, movie_script):\n",
    "        return self.preprocessor.preprocess(movie_script)\n",
    "    \n",
    "    def check_keyword_frequency(self, raw_keywords):\n",
    "        keyword_dict = {key: 0 for key in list(set(raw_keywords))}\n",
    "        \n",
    "        for keyword in raw_keywords:\n",
    "            keyword_dict[keyword] += 1\n",
    "\n",
    "        return keyword_dict\n",
    "    \n",
    "    def get_inverted_index(self, movie_raw):\n",
    "        index = {}\n",
    "        doc_len = len(movie_raw)\n",
    "        \n",
    "        for i, movie_script in enumerate(movie_raw):\n",
    "            key_list = self.script_to_key_list(movie_script)\n",
    "            key_dict = self.check_keyword_frequency(key_list)\n",
    "            \n",
    "            key_list = list(set(key_list))  # make unique set\n",
    "            \n",
    "            for key in key_list:            \n",
    "                h = [] \n",
    "                tf = 1 + math.log(key_dict[key], 2)\n",
    "                h.append((tf, i))\n",
    "\n",
    "                if key in index:\n",
    "                    index[key][1].extend(h)\n",
    "                else:\n",
    "                    index[key] = (key_dict[key], h)\n",
    "                \n",
    "        return index;\n",
    "    \n",
    "        \n",
    "    def parse_to_tf_idf(self, term):\n",
    "        if (term in self.index) is False:\n",
    "            return None\n",
    "        \n",
    "        (df, tf_list) = self.index[term]\n",
    "        idf = math.log(self.doc_len/len(tf_list),2)\n",
    "        \n",
    "        return [(item[0] * idf, item[1]) for item in tf_list]\n",
    "                             \n",
    "    def calc_similarity(self, term_list):\n",
    "        doc = {}\n",
    "        for term in term_list:\n",
    "            tf_idf_list = self.parse_to_tf_idf(term)\n",
    "            \n",
    "            if tf_idf_list != None:\n",
    "                for tf_idf, di in tf_idf_list:\n",
    "                    if di in doc:\n",
    "                        doc[di] += tf_idf\n",
    "                    else:\n",
    "                        doc[di] = tf_idf\n",
    "        return doc\n",
    "    \n",
    "    def query(self, text, k):\n",
    "\n",
    "        term_list = self.script_to_key_list(text)\n",
    " \n",
    "        docs = self.calc_similarity(self.check_keyword_frequency(term_list))\n",
    "\n",
    "        return heapq.nlargest(k,docs.items(), key=itemgetter(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe4f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = \"a's, able, im, uh, about, cod't, cont'd, above, according, accordingly, across, actually, after, afterwards, again, against, ain’t, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, appear, appreciate, appropriate, are, aren’t, around, as, aside, ask, asking, associated, at, available, away, awfully, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, both, brief, but, by, c’mon, c’s, came, can, can’t, cannot, cant, cause, causes, certain, certainly, changes, clearly, co, com, come, comes, concerning, consequently, consider, considering, contain, containing, contains, corresponding, could, couldn’t, course, currently, definitely, described, despite, did, didn’t, different, do, does, doesn’t, doing, don’t, done, down, downwards, during, each, edu, eg, eight, either, else, elsewhere, enough, entirely, especially, et, etc, even, ever, every, everybody, everyone, everything, everywhere, ex, exactly, example, except, far, few, fifth, first, five, followed, following, follows, for, former, formerly, forth, four, from, further, furthermore, get, gets, getting, given, gives, go, goes, going, gone, got, gotten, greetings, had, hadn’t, happens, hardly, has, hasn’t, have, haven’t, having, he, he’s, hello, help, hence, her, here, here’s, hereafter, hereby, herein, hereupon, hers, herself, hi, him, himself, his, hither, hopefully, how, howbeit, however, i, i’d, i’ll, i’m, i’ve, ie, if, ignored, immediate, in, inasmuch, inc, indeed, indicate, indicated, indicates, inner, insofar, instead, into, inward, is, isn’t, it, it’d, it’ll, it’s, its, itself, just, keep, keeps, kept, know, knows, known, last, lately, later, latter, latterly, least, less, lest, let, let’s, like, liked, likely, little, look, looking, looks, ltd, mainly, many, may, maybe, me, mean, meanwhile, merely, might, more, moreover, most, mostly, much, must, my, myself, name, namely, nd, near, nearly, necessary, need, needs, neither, never, nevertheless, new, next, nine, no, nobody, non, none, noone, nor, normally, not, nothing, novel, now, nowhere, obviously, of, off, often, oh, ok, okay, old, on, once, one, ones, only, onto, or, other, others, otherwise, ought, our, ours, ourselves, out, outside, over, overall, own, particular, particularly, per, perhaps, placed, please, plus, possible, presumably, probably, provides, que, quite, qv, rather, rd, re, really, reasonably, regarding, regardless, regards, relatively, respectively, right, said, same, saw, say, saying, says, second, secondly, see, seeing, seem, seemed, seeming, seems, seen, self, selves, sensible, sent, serious, seriously, seven, several, shall, she, should, shouldn’t, since, six, so, some, somebody, somehow, someone, something, sometime, sometimes, somewhat, somewhere, soon, sorry, specified, specify, specifying, still, sub, such, sup, sure, t’s, take, taken, tell, tends, th, than, thank, thanks, thanx, that, that’s, thats, the, their, theirs, them, themselves, then, thence, there, there’s, thereafter, thereby, therefore, therein, theres, thereupon, these, they, they’d, they’ll, they’re, they’ve, think, third, this, thorough, thoroughly, those, though, three, through, throughout, thru, thus, to, together, too, took, toward, towards, tried, tries, truly, try, trying, twice, two, un, under, unfortunately, unless, unlikely, until, unto, up, upon, us, use, used, useful, uses, using, usually, value, various, very, via, viz, vs, want, wants, was, wasn’t, way, we, we’d, we’ll, we’re, we’ve, welcome, well, went, were, weren’t, what, what’s, whatever, when, whence, whenever, where, where’s, whereafter, whereas, whereby, wherein, whereupon, wherever, whether, which, while, whither, who, who’s, whoever, whole, whom, whose, why, will, willing, wish, with, within, without, won't, wonder, would, would, wouldn't, yes, yet, you, you'd, you'll, you're, you've, your, yours, yourself, yourselves, zero\".replace('’',\"'\").split(', ')\n",
    "\n",
    "end_words = '|'.join(['\\,','\\.','\\?','\\!','\\-','\\:','\\;','\\r','\\n','\\t','\\(','\\)','\\{','\\}',\"\\'s\",\"\\_\",\"\\&\", \"\\#\", \"\\@\", \"\\#\",\"\\^\",\"\\*\",\"\\/\",\"\\[\",\"\\]\"])\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec53a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    \n",
    "    def __init__(self, stop_words, end_words, stemmer):\n",
    "        self.stop_words = stop_words\n",
    "        self.end_words = end_words\n",
    "        self.stemmer = stemmer\n",
    "        \n",
    "    def preprocess(self, raw_data):\n",
    "        tokenized_data = self.tokenize(raw_data)\n",
    "        stemmed_data = self.stemming(tokenized_data)\n",
    "        \n",
    "        return stemmed_data\n",
    "    \n",
    "    def tokenize(self, data):\n",
    "        # replace end words with space\n",
    "        end_replaced = re.sub(self.end_words, \" \", data.lower())\n",
    "        \n",
    "        # split\n",
    "        script_chunks = end_replaced.split(' ')\n",
    "        \n",
    "        # remove stop_wrods\n",
    "        filtered_script = filter(lambda x: len(x) > 1 and ((x in stop_words) is False), script_chunks)\n",
    "        \n",
    "        return [\"{}\".format(key.replace('\"', \"\").replace(\"'\",\"\")) for key in list(filtered_script)]\n",
    "    \n",
    "    def stemming(self, words):\n",
    "        return [self.stemmer.stem(word) for word in words]\n",
    "    \n",
    "    def check_keyword_frequency(self, raw_keywords):\n",
    "        keyword_dict = {key: 0 for key in list(set(raw_keywords))}\n",
    "        \n",
    "        for keyword in raw_keywords:\n",
    "            keyword_dict[keyword] += 1\n",
    "\n",
    "        return keyword_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e475f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Preprocessor(stop_words=stop_words, end_words=end_words, stemmer=PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12687ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('./data/grouped_by_genre/raw/Musical.json', 'r')\n",
    "movie_scripts = json.loads(f.read())\n",
    "movie_scripts = [movie[1] for movie in movie_scripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255bad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13155\n"
     ]
    }
   ],
   "source": [
    "a = VSModel(movie_scripts, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f45fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 1.1567397949489706),\n",
       " (16, 1.0919830477114352),\n",
       " (2, 1.0862145472853995),\n",
       " (23, 1.0742536406419303),\n",
       " (0, 1.0270936794498897),\n",
       " (26, 0.9778992671054729),\n",
       " (15, 0.9590322711876736),\n",
       " (25, 0.9590322711876736),\n",
       " (18, 0.9385912733496897),\n",
       " (8, 0.9162895458430871)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.query('i love you',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdf035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1f44e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
